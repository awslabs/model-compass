# Complete Multi-Provider Configuration Example
# This example shows how to configure multiple providers (OpenAI, Anthropic, Ollama)
# in a single configuration with intelligent fallbacks and environment-specific profiles.

default_profile: "balanced"

models:
  # OpenAI Models
  gpt-4:
    provider: openai
    model: gpt-4
    metadata:
      reasoning: high
      cost: high
      speed: medium
      capabilities: [chat, completion, function_calling]
      context_length: 8192
      multimodal: false
      deployment: cloud
    parameters:
      temperature: 0.7
      max_tokens: 2048

  gpt-3.5-turbo:
    provider: openai
    model: gpt-3.5-turbo
    metadata:
      reasoning: medium
      cost: low
      speed: fast
      capabilities: [chat, completion, function_calling]
      context_length: 16385
      multimodal: false
      deployment: cloud
    parameters:
      temperature: 0.7
      max_tokens: 2048

  # Anthropic Models
  claude-3-sonnet:
    provider: anthropic
    model: claude-3-sonnet-20240229
    metadata:
      reasoning: high
      cost: medium
      speed: medium
      capabilities: [chat, completion, vision]
      context_length: 200000
      multimodal: true
      deployment: cloud
    parameters:
      temperature: 0.7
      max_tokens: 4096

  claude-3-haiku:
    provider: anthropic
    model: claude-3-haiku-20240307
    metadata:
      reasoning: medium
      cost: low
      speed: fast
      capabilities: [chat, completion, vision]
      context_length: 200000
      multimodal: true
      deployment: cloud
    parameters:
      temperature: 0.7
      max_tokens: 2048

  # Ollama Local Models
  llama3:
    provider: ollama
    model: llama3
    metadata:
      reasoning: high
      cost: free
      speed: medium
      capabilities: [chat, completion]
      context_length: 8192
      multimodal: false
      deployment: local
      local: true
    parameters:
      temperature: 0.7
      num_predict: 2048

  codellama:
    provider: ollama
    model: codellama
    metadata:
      reasoning: medium
      cost: free
      speed: medium
      capabilities: [chat, completion, code]
      context_length: 16384
      multimodal: false
      deployment: local
      local: true
    parameters:
      temperature: 0.1
      num_predict: 2048

providers:
  openai:
    base_url: https://api.openai.com/v1
    timeout: 30
    headers:
      Authorization: "Bearer ${OPENAI_API_KEY}"
      Content-Type: application/json

  anthropic:
    base_url: https://api.anthropic.com
    timeout: 60
    headers:
      x-api-key: "${ANTHROPIC_API_KEY}"
      Content-Type: application/json
      anthropic-version: "2023-06-01"

  ollama:
    base_url: http://localhost:11434
    timeout: 120
    headers:
      Content-Type: application/json

profiles:
  # Environment-based profiles
  development:
    model: llama3  # Use local model for development
    parameters:
      temperature: 0.9
      num_predict: 1024

  staging:
    model: claude-3-haiku  # Fast cloud model for staging
    parameters:
      temperature: 0.5
      max_tokens: 2048

  production:
    model: gpt-4  # Best quality for production
    parameters:
      temperature: 0.1
      max_tokens: 2048

  # Performance-based profiles
  fast:
    model: gpt-3.5-turbo  # Fastest cloud option
    parameters:
      temperature: 0.3
      max_tokens: 1024

  balanced:
    model: claude-3-sonnet  # Good balance of speed/quality
    parameters:
      temperature: 0.7
      max_tokens: 2048

  powerful:
    model: gpt-4  # Best reasoning
    parameters:
      temperature: 0.7
      max_tokens: 4096

  # Task-specific profiles
  coding:
    model: codellama  # Code-specialized local model
    parameters:
      temperature: 0.1
      num_predict: 2048

  creative:
    model: gpt-4  # Best for creative tasks
    parameters:
      temperature: 0.9
      max_tokens: 4096

  analysis:
    model: claude-3-sonnet  # Good for analysis with large context
    parameters:
      temperature: 0.3
      max_tokens: 4096

  vision:
    model: claude-3-sonnet  # Multimodal capabilities
    parameters:
      temperature: 0.5
      max_tokens: 2048

  # Cost-conscious profiles
  free:
    model: llama3  # Local model, no API costs
    parameters:
      temperature: 0.7
      num_predict: 2048

  cheap:
    model: gpt-3.5-turbo  # Cheapest cloud option
    parameters:
      temperature: 0.7
      max_tokens: 1024

aliases:
  # Environment aliases
  dev: development
  stage: staging
  prod: production

  # Performance aliases
  quick: fast
  best: powerful
  smart: balanced

  # Deployment aliases
  local: llama3
  cloud: gpt-4
  hybrid: balanced

  # Task aliases
  coder: coding
  writer: creative
  researcher: analysis
  viewer: vision

  # Cost aliases
  budget: free
  economy: cheap
  premium: powerful

  # Fallback chains
  primary: gpt-4
  secondary: claude-3-sonnet
  tertiary: gpt-3.5-turbo
  emergency: llama3

  # Provider-specific aliases
  openai-best: gpt-4
  openai-fast: gpt-3.5-turbo
  anthropic-best: claude-3-sonnet
  anthropic-fast: claude-3-haiku
  ollama-best: llama3
  ollama-code: codellama